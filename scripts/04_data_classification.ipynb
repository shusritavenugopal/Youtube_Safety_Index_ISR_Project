{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn nltk textstat vaderSentiment better-profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier  # Example classifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Text analysis libraries\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import textstat\n",
    "from better_profanity import profanity\n",
    "\n",
    "# Other utilities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # To suppress warnings (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preparation\n",
    "In this step, we'll:\n",
    "\n",
    "1. Load the dataset.\n",
    "2. Check for missing values or duplicates.\n",
    "3. Encode the target column (is_kids_safe) if necessary.\n",
    "4. Split the data into features (X) and target (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>tags</th>\n",
       "      <th>category_id</th>\n",
       "      <th>Category Label</th>\n",
       "      <th>default_audio_language</th>\n",
       "      <th>transcript</th>\n",
       "      <th>...</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>text_dict</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>kids_safe_nltk</th>\n",
       "      <th>profanity</th>\n",
       "      <th>themes</th>\n",
       "      <th>readability</th>\n",
       "      <th>tone</th>\n",
       "      <th>cultural_sensitivity</th>\n",
       "      <th>is_kids_safe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R7KfBjoLGOY</td>\n",
       "      <td>Every Swear Word RANKED...</td>\n",
       "      <td>Every Swear Word RANKED... today we rank every...</td>\n",
       "      <td>2024-06-15T20:34:45Z</td>\n",
       "      <td>The Duck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>swear words a very essential part of the vocab...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'Every Swear Word RANKED...', 'descr...</td>\n",
       "      <td>Every Swear Word RANKED... Every Swear Word RA...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GkQmkZNLCeM</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>2021-07-08T18:29:49Z</td>\n",
       "      <td>Genevieve's Playhouse - Learning Videos for Kids</td>\n",
       "      <td>best, learning video for toddlers, for kids, c...</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcript not available</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'Best Toy Learning Video for Toddler...</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_et1i-ykdxA</td>\n",
       "      <td>Best ABC Learning Toy Video for Toddlers! Paw ...</td>\n",
       "      <td>Best ABC Learning Toy Video for Toddlers! Paw ...</td>\n",
       "      <td>2022-07-14T16:45:05Z</td>\n",
       "      <td>Genevieve's Playhouse - Learning Videos for Kids</td>\n",
       "      <td>abc, for toddlers, for baby, learning, educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcript not available</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'Best ABC Learning Toy Video for Tod...</td>\n",
       "      <td>Best ABC Learning Toy Video for Toddlers! Paw ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  R7KfBjoLGOY                         Every Swear Word RANKED...   \n",
       "1  GkQmkZNLCeM  Best Toy Learning Video for Toddlers and Kids ...   \n",
       "2  _et1i-ykdxA  Best ABC Learning Toy Video for Toddlers! Paw ...   \n",
       "\n",
       "                                         description          published_at  \\\n",
       "0  Every Swear Word RANKED... today we rank every...  2024-06-15T20:34:45Z   \n",
       "1  Best Toy Learning Video for Toddlers and Kids ...  2021-07-08T18:29:49Z   \n",
       "2  Best ABC Learning Toy Video for Toddlers! Paw ...  2022-07-14T16:45:05Z   \n",
       "\n",
       "                                      channel_title  \\\n",
       "0                                          The Duck   \n",
       "1  Genevieve's Playhouse - Learning Videos for Kids   \n",
       "2  Genevieve's Playhouse - Learning Videos for Kids   \n",
       "\n",
       "                                                tags  category_id  \\\n",
       "0                                                NaN            1   \n",
       "1  best, learning video for toddlers, for kids, c...            1   \n",
       "2  abc, for toddlers, for baby, learning, educati...            1   \n",
       "\n",
       "     Category Label default_audio_language  \\\n",
       "0  Film & Animation                    NaN   \n",
       "1  Film & Animation                     en   \n",
       "2  Film & Animation                     en   \n",
       "\n",
       "                                          transcript  ... dislike_count  \\\n",
       "0  swear words a very essential part of the vocab...  ...           NaN   \n",
       "1                           Transcript not available  ...           NaN   \n",
       "2                           Transcript not available  ...           NaN   \n",
       "\n",
       "                                           text_dict  \\\n",
       "0  {'title': 'Every Swear Word RANKED...', 'descr...   \n",
       "1  {'title': 'Best Toy Learning Video for Toddler...   \n",
       "2  {'title': 'Best ABC Learning Toy Video for Tod...   \n",
       "\n",
       "                                       combined_text  kids_safe_nltk  \\\n",
       "0  Every Swear Word RANKED... Every Swear Word RA...               0   \n",
       "1  Best Toy Learning Video for Toddlers and Kids ...               1   \n",
       "2  Best ABC Learning Toy Video for Toddlers! Paw ...               1   \n",
       "\n",
       "  profanity themes  readability  tone  cultural_sensitivity  is_kids_safe  \n",
       "0      True   True        False  True                 False         False  \n",
       "1     False  False        False  True                 False          True  \n",
       "2     False  False        False  True                 False          True  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel('../data/raw/final_dataset_classification.xlsx')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Check for Missing Values and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " video_id                     0\n",
      "title                        0\n",
      "description                483\n",
      "published_at                 0\n",
      "channel_title                0\n",
      "tags                      1026\n",
      "category_id                  0\n",
      "Category Label             397\n",
      "default_audio_language    1301\n",
      "transcript                 584\n",
      "duration                  2038\n",
      "view_count                2038\n",
      "like_count                2038\n",
      "dislike_count             2038\n",
      "text_dict                    0\n",
      "combined_text                0\n",
      "kids_safe_nltk               0\n",
      "profanity                    0\n",
      "themes                       0\n",
      "readability                  0\n",
      "tone                         0\n",
      "cultural_sensitivity         0\n",
      "is_kids_safe                 0\n",
      "dtype: int64\n",
      "Number of duplicates: 461\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Confirm duplicates are removed\n",
    "print(\"Number of duplicates after cleaning:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the Target Column (if not binary integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>tags</th>\n",
       "      <th>category_id</th>\n",
       "      <th>Category Label</th>\n",
       "      <th>default_audio_language</th>\n",
       "      <th>transcript</th>\n",
       "      <th>...</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>text_dict</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>kids_safe_nltk</th>\n",
       "      <th>profanity</th>\n",
       "      <th>themes</th>\n",
       "      <th>readability</th>\n",
       "      <th>tone</th>\n",
       "      <th>cultural_sensitivity</th>\n",
       "      <th>is_kids_safe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R7KfBjoLGOY</td>\n",
       "      <td>Every Swear Word RANKED...</td>\n",
       "      <td>Every Swear Word RANKED... today we rank every...</td>\n",
       "      <td>2024-06-15T20:34:45Z</td>\n",
       "      <td>The Duck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>swear words a very essential part of the vocab...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'Every Swear Word RANKED...', 'descr...</td>\n",
       "      <td>Every Swear Word RANKED... Every Swear Word RA...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GkQmkZNLCeM</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>2021-07-08T18:29:49Z</td>\n",
       "      <td>Genevieve's Playhouse - Learning Videos for Kids</td>\n",
       "      <td>best, learning video for toddlers, for kids, c...</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcript not available</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'Best Toy Learning Video for Toddler...</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_et1i-ykdxA</td>\n",
       "      <td>Best ABC Learning Toy Video for Toddlers! Paw ...</td>\n",
       "      <td>Best ABC Learning Toy Video for Toddlers! Paw ...</td>\n",
       "      <td>2022-07-14T16:45:05Z</td>\n",
       "      <td>Genevieve's Playhouse - Learning Videos for Kids</td>\n",
       "      <td>abc, for toddlers, for baby, learning, educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcript not available</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'Best ABC Learning Toy Video for Tod...</td>\n",
       "      <td>Best ABC Learning Toy Video for Toddlers! Paw ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_UNe6tz28OI</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>2021-03-23T01:43:53Z</td>\n",
       "      <td>Genevieve's Playhouse - Learning Videos for Kids</td>\n",
       "      <td>best, toddler learning videos, for kids, for b...</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcript not available</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'Best Toy Learning Video for Toddler...</td>\n",
       "      <td>Best Toy Learning Video for Toddlers and Kids ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zamt_1PAwqE</td>\n",
       "      <td>The Joy of Sharing Short Episode |  Fun Learni...</td>\n",
       "      <td>Welcome to \"The Learning Show\", the exciting a...</td>\n",
       "      <td>2023-05-19T14:42:38Z</td>\n",
       "      <td>Videogyan Shows - Educational Videos For Kids</td>\n",
       "      <td>shorts, learning for kids, educational videos,...</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcript not available</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'title': 'The Joy of Sharing Short Episode | ...</td>\n",
       "      <td>The Joy of Sharing Short Episode |  Fun Learni...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>8_nv0q5ztYE</td>\n",
       "      <td>Pitch Perfect - Trailer</td>\n",
       "      <td>Arriving at her new college, Beca (Anna Kendri...</td>\n",
       "      <td>2013-10-11T05:10:52Z</td>\n",
       "      <td>Universal Pictures All-Access</td>\n",
       "      <td>Anna Kendrick, Brittany Snow, Anna Camp, Rebel...</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi there welcome to Bon University here's your...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'title': 'Pitch Perfect - Trailer', 'descript...</td>\n",
       "      <td>Pitch Perfect - Trailer Arriving at her new co...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>irhVziMbSZk</td>\n",
       "      <td>Moneyball (2011) - Trailer</td>\n",
       "      <td>Real-life story is based on Oakland A's genera...</td>\n",
       "      <td>2014-07-11T03:41:39Z</td>\n",
       "      <td>Sony Pictures at Home UK</td>\n",
       "      <td>Philip Seymour Hoffman, Brad Pitt, Robin Wrigh...</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'title': 'Moneyball (2011) - Trailer', 'descr...</td>\n",
       "      <td>Moneyball (2011) - Trailer Real-life story is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>dMaq_pfxs-0</td>\n",
       "      <td>Hitch (2005) - Trailer</td>\n",
       "      <td>New York \"date doctor\" Alex Hitchens (Will Smi...</td>\n",
       "      <td>2014-02-01T23:07:44Z</td>\n",
       "      <td>Sony Pictures Home Entertainment</td>\n",
       "      <td>Kevin James, Eva Mendes, Michael Rapaport, Ada...</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'title': 'Hitch (2005) - Trailer', 'descripti...</td>\n",
       "      <td>Hitch (2005) - Trailer New York \"date doctor\" ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>Ecn8dvDK7LQ</td>\n",
       "      <td>Lemonade Mouth - Trailer</td>\n",
       "      <td>Be heard. Be strong. Be proud. Its time to tur...</td>\n",
       "      <td>2012-07-11T16:50:42Z</td>\n",
       "      <td>DisneyMoviesOnDemand</td>\n",
       "      <td>Lemonade Mouth, Bridgit Mendler, Adam Hicks, H...</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'title': 'Lemonade Mouth - Trailer', 'descrip...</td>\n",
       "      <td>Lemonade Mouth - Trailer Be heard. Be strong. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>0JxL-inCPbc</td>\n",
       "      <td>Tinker, Tailor, Soldier, Spy - Trailer</td>\n",
       "      <td>Gary Oldman leads a stunning all-star cast in ...</td>\n",
       "      <td>2014-03-12T04:53:38Z</td>\n",
       "      <td>Universal Pictures All-Access</td>\n",
       "      <td>Tinker, Tailor, Soldier, Spy, Film, Film &amp; Ani...</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'title': 'Tinker, Tailor, Soldier, Spy - Trai...</td>\n",
       "      <td>Tinker, Tailor, Soldier, Spy - Trailer Gary Ol...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2964 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                              title  \\\n",
       "0     R7KfBjoLGOY                         Every Swear Word RANKED...   \n",
       "1     GkQmkZNLCeM  Best Toy Learning Video for Toddlers and Kids ...   \n",
       "2     _et1i-ykdxA  Best ABC Learning Toy Video for Toddlers! Paw ...   \n",
       "3     _UNe6tz28OI  Best Toy Learning Video for Toddlers and Kids ...   \n",
       "4     zamt_1PAwqE  The Joy of Sharing Short Episode |  Fun Learni...   \n",
       "...           ...                                                ...   \n",
       "3419  8_nv0q5ztYE                            Pitch Perfect - Trailer   \n",
       "3420  irhVziMbSZk                         Moneyball (2011) - Trailer   \n",
       "3421  dMaq_pfxs-0                             Hitch (2005) - Trailer   \n",
       "3422  Ecn8dvDK7LQ                           Lemonade Mouth - Trailer   \n",
       "3424  0JxL-inCPbc             Tinker, Tailor, Soldier, Spy - Trailer   \n",
       "\n",
       "                                            description          published_at  \\\n",
       "0     Every Swear Word RANKED... today we rank every...  2024-06-15T20:34:45Z   \n",
       "1     Best Toy Learning Video for Toddlers and Kids ...  2021-07-08T18:29:49Z   \n",
       "2     Best ABC Learning Toy Video for Toddlers! Paw ...  2022-07-14T16:45:05Z   \n",
       "3     Best Toy Learning Video for Toddlers and Kids ...  2021-03-23T01:43:53Z   \n",
       "4     Welcome to \"The Learning Show\", the exciting a...  2023-05-19T14:42:38Z   \n",
       "...                                                 ...                   ...   \n",
       "3419  Arriving at her new college, Beca (Anna Kendri...  2013-10-11T05:10:52Z   \n",
       "3420  Real-life story is based on Oakland A's genera...  2014-07-11T03:41:39Z   \n",
       "3421  New York \"date doctor\" Alex Hitchens (Will Smi...  2014-02-01T23:07:44Z   \n",
       "3422  Be heard. Be strong. Be proud. Its time to tur...  2012-07-11T16:50:42Z   \n",
       "3424  Gary Oldman leads a stunning all-star cast in ...  2014-03-12T04:53:38Z   \n",
       "\n",
       "                                         channel_title  \\\n",
       "0                                             The Duck   \n",
       "1     Genevieve's Playhouse - Learning Videos for Kids   \n",
       "2     Genevieve's Playhouse - Learning Videos for Kids   \n",
       "3     Genevieve's Playhouse - Learning Videos for Kids   \n",
       "4        Videogyan Shows - Educational Videos For Kids   \n",
       "...                                                ...   \n",
       "3419                     Universal Pictures All-Access   \n",
       "3420                          Sony Pictures at Home UK   \n",
       "3421                  Sony Pictures Home Entertainment   \n",
       "3422                              DisneyMoviesOnDemand   \n",
       "3424                     Universal Pictures All-Access   \n",
       "\n",
       "                                                   tags  category_id  \\\n",
       "0                                                   NaN            1   \n",
       "1     best, learning video for toddlers, for kids, c...            1   \n",
       "2     abc, for toddlers, for baby, learning, educati...            1   \n",
       "3     best, toddler learning videos, for kids, for b...            1   \n",
       "4     shorts, learning for kids, educational videos,...            1   \n",
       "...                                                 ...          ...   \n",
       "3419  Anna Kendrick, Brittany Snow, Anna Camp, Rebel...           44   \n",
       "3420  Philip Seymour Hoffman, Brad Pitt, Robin Wrigh...           44   \n",
       "3421  Kevin James, Eva Mendes, Michael Rapaport, Ada...           44   \n",
       "3422  Lemonade Mouth, Bridgit Mendler, Adam Hicks, H...           44   \n",
       "3424  Tinker, Tailor, Soldier, Spy, Film, Film & Ani...           44   \n",
       "\n",
       "        Category Label default_audio_language  \\\n",
       "0     Film & Animation                    NaN   \n",
       "1     Film & Animation                     en   \n",
       "2     Film & Animation                     en   \n",
       "3     Film & Animation                     en   \n",
       "4     Film & Animation                     en   \n",
       "...                ...                    ...   \n",
       "3419               NaN                    NaN   \n",
       "3420               NaN                    NaN   \n",
       "3421               NaN                     en   \n",
       "3422               NaN                    NaN   \n",
       "3424               NaN                    NaN   \n",
       "\n",
       "                                             transcript  ... dislike_count  \\\n",
       "0     swear words a very essential part of the vocab...  ...           NaN   \n",
       "1                              Transcript not available  ...           NaN   \n",
       "2                              Transcript not available  ...           NaN   \n",
       "3                              Transcript not available  ...           NaN   \n",
       "4                              Transcript not available  ...           NaN   \n",
       "...                                                 ...  ...           ...   \n",
       "3419  hi there welcome to Bon University here's your...  ...           0.0   \n",
       "3420                                                NaN  ...           0.0   \n",
       "3421                                                NaN  ...           0.0   \n",
       "3422                                                NaN  ...           0.0   \n",
       "3424                                                NaN  ...           0.0   \n",
       "\n",
       "                                              text_dict  \\\n",
       "0     {'title': 'Every Swear Word RANKED...', 'descr...   \n",
       "1     {'title': 'Best Toy Learning Video for Toddler...   \n",
       "2     {'title': 'Best ABC Learning Toy Video for Tod...   \n",
       "3     {'title': 'Best Toy Learning Video for Toddler...   \n",
       "4     {'title': 'The Joy of Sharing Short Episode | ...   \n",
       "...                                                 ...   \n",
       "3419  {'title': 'Pitch Perfect - Trailer', 'descript...   \n",
       "3420  {'title': 'Moneyball (2011) - Trailer', 'descr...   \n",
       "3421  {'title': 'Hitch (2005) - Trailer', 'descripti...   \n",
       "3422  {'title': 'Lemonade Mouth - Trailer', 'descrip...   \n",
       "3424  {'title': 'Tinker, Tailor, Soldier, Spy - Trai...   \n",
       "\n",
       "                                          combined_text  kids_safe_nltk  \\\n",
       "0     Every Swear Word RANKED... Every Swear Word RA...               0   \n",
       "1     Best Toy Learning Video for Toddlers and Kids ...               1   \n",
       "2     Best ABC Learning Toy Video for Toddlers! Paw ...               1   \n",
       "3     Best Toy Learning Video for Toddlers and Kids ...               1   \n",
       "4     The Joy of Sharing Short Episode |  Fun Learni...               0   \n",
       "...                                                 ...             ...   \n",
       "3419  Pitch Perfect - Trailer Arriving at her new co...               0   \n",
       "3420  Moneyball (2011) - Trailer Real-life story is ...               1   \n",
       "3421  Hitch (2005) - Trailer New York \"date doctor\" ...               1   \n",
       "3422  Lemonade Mouth - Trailer Be heard. Be strong. ...               1   \n",
       "3424  Tinker, Tailor, Soldier, Spy - Trailer Gary Ol...               0   \n",
       "\n",
       "     profanity themes  readability  tone  cultural_sensitivity  is_kids_safe  \n",
       "0         True   True        False  True                 False         False  \n",
       "1        False  False        False  True                 False          True  \n",
       "2        False  False        False  True                 False          True  \n",
       "3        False  False        False  True                 False          True  \n",
       "4        False   True        False  True                 False          True  \n",
       "...        ...    ...          ...   ...                   ...           ...  \n",
       "3419      True   True        False  True                 False          True  \n",
       "3420     False  False        False  True                 False         False  \n",
       "3421     False  False         True  True                 False         False  \n",
       "3422     False  False        False  True                 False          True  \n",
       "3424     False   True        False  True                 False          True  \n",
       "\n",
       "[2964 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure target column is binary\n",
    "df['kids_safe_nltk'] = df['kids_safe_nltk'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_equal = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Features (X) and Target (y)\n",
    "Separate the combined_text column as the feature and is_kids_safe as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'title', 'description', 'published_at', 'channel_title',\n",
       "       'tags', 'category_id', 'Category Label', 'default_audio_language',\n",
       "       'transcript', 'duration', 'view_count', 'like_count', 'dislike_count',\n",
       "       'text_dict', 'combined_text', 'kids_safe_nltk', 'profanity', 'themes',\n",
       "       'readability', 'tone', 'cultural_sensitivity', 'is_kids_safe'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and target (y)\n",
    "X = df['combined_text']  # Feature column\n",
    "y = df['kids_safe_nltk']   # Target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## used for increasing BERT model accuracy (skip otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and target (y)\n",
    "X = df_equal['combined_text']  # Feature column\n",
    "y = df_equal['kids_safe_nltk']   # Target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Text Preprocessing\n",
    "Since X contains text data, we need to preprocess it before feeding it into a machine learning model. This involves:\n",
    "\n",
    "1. Lowercasing: Converting text to lowercase for consistency.\n",
    "2. Removing Special Characters: Eliminating punctuation, numbers, and special characters.\n",
    "3. Tokenization: Splitting text into individual words.\n",
    "4. Removing Stop Words: Removing common words like \"the\", \"is\", etc., that donâ€™t add much meaning.\n",
    "5. Stemming/Lemmatization: Reducing words to their root form (e.g., \"running\" â†’ \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shusritavenugopal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shusritavenugopal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shusritavenugopal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    every swear word ranked every swear word ranke...\n",
      "1    best toy learning video toddler kid learn colo...\n",
      "2    best abc learning toy video toddler paw patrol...\n",
      "3    best toy learning video toddler kid learn colo...\n",
      "4    joy sharing short episode fun learning video c...\n",
      "Name: combined_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Rejoin tokens into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to all text in X\n",
    "X = X.apply(preprocess_text)\n",
    "\n",
    "# Check a few samples after preprocessing\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train-Test Split and Vectorization\n",
    "## 4.1 Train-Test Split\n",
    "1. Training Data: Used to train the model.\n",
    "2. Testing Data: Used to evaluate the model's performance.\n",
    "\n",
    "## 4.2 Text Vectorization\n",
    "Since machine learning models work with numerical data, weâ€™ll convert the text into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency). This method represents the importance of words in a document relative to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (2371, 5000)\n",
      "Test Data Shape: (593, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer on the training data and transform both train and test data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Check the shape of the transformed data\n",
    "print(\"Train Data Shape:\", X_train_tfidf.shape)\n",
    "print(\"Test Data Shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data has been successfully converted into numerical form using TF-IDF vectorization. \n",
    "\n",
    "1. Train Data Shape: (2371, 5000)\n",
    "- 2371 refers to the number of training samples (rows).\n",
    "- 5000 refers to the number of features (columns), which corresponds to the top 5000 terms (words or n-grams) that the TF-IDF vectorizer has identified as most significant in your dataset.\n",
    "2. Test Data Shape: (593, 5000)\n",
    "- 593 refers to the number of test samples (rows).\n",
    "- 5000 remains the same, as the test data is transformed using the same set of features (top 5000 terms) that the model learned from the training data.\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "The train-test split worked as expected, with 80% of the data (2371 samples) used for training and 20% (593 samples) for testing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model Training\n",
    "\n",
    "In this step, we will train a classifier using the features extracted by the TF-IDF vectorizer. A good starting point for text classification tasks is the Logistic Regression model, as it's simple and effective for binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7639123102866779\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84       398\n",
      "           1       0.72      0.47      0.57       195\n",
      "\n",
      "    accuracy                           0.76       593\n",
      "   macro avg       0.75      0.69      0.70       593\n",
      "weighted avg       0.76      0.76      0.75       593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing Logistic Regression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score: 0.76 (or 76%) indicates that the model is correctly classifying 76% of the test instances.\n",
    "\n",
    "#### Precision:\n",
    "- For class 0 (kids safe): Precision of 0.78 means that 78% of instances predicted as kids-safe are actually kids-safe.\n",
    "- For class 1 (not safe): Precision of 0.72 means that 72% of instances predicted as not safe are indeed not safe.\n",
    "\n",
    "#### Recall:\n",
    "- For class 0 (kids safe): Recall of 0.91 means that 91% of all actual kids-safe instances were correctly identified by the model.\n",
    "- For class 1 (not safe): Recall of 0.47 means that only 47% of the actual not-safe instances were correctly identified.\n",
    "#### F1-Score:\n",
    "- For class 0: The F1-score is 0.84, which is a balance between precision and recall.\n",
    "- For class 1: The F1-score is 0.57, indicating that the model struggles more with predicting the \"not safe\" class accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression model is performing well in identifying \"safe for kids\" content. But it struggles more with identifying not-safe content (lower recall for class 1). So, choosing to try other models like Random Forest, SVM, or XGBoost to see if they perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing the SVM model and other required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize and train the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model using the transformed features\n",
    "svm_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make predictions and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7993254637436762\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       398\n",
      "           1       0.71      0.66      0.68       195\n",
      "\n",
      "    accuracy                           0.80       593\n",
      "   macro avg       0.77      0.76      0.77       593\n",
      "weighted avg       0.80      0.80      0.80       593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model achieved an accuracy of 80%, which is a solid result. \n",
    "- Class 0 (not kid-safe): The model performed well in this class, with a precision of 0.84, recall of 0.87, and an F1-score of 0.85. This indicates that the model is correctly identifying most of the instances in this class.\n",
    "- Class 1 (kid-safe): The performance here is slightly lower, with a precision of 0.71, recall of 0.66, and F1-score of 0.68. The model is still performing decently, but thereâ€™s room for improvement in identifying kid-safe content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning can help improve the performance of your SVM model, especially when adjusting the C parameter, which controls the trade-off between a smooth decision boundary and classifying training points correctly.\n",
    "\n",
    "#### Step 1: Define a parameter grid: test different values for the C parameter, which determines the regularization strength.\n",
    "\n",
    "#### Step 2: GridSearchCV will exhaustively search through all combinations of parameters and return the best one based on cross-validation performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   1.6s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   1.8s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   1.8s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   1.8s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   1.5s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   1.5s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   1.7s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   1.6s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   1.3s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   1.8s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   1.5s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   1.5s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   1.4s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   1.4s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   1.4s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   1.4s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   1.3s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   1.3s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   1.3s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   1.9s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   1.9s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   1.9s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   1.9s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   1.3s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   1.4s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   1.4s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   1.2s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   2.1s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   1.7s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   1.6s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   1.6s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   1.6s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   1.7s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   2.2s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   2.3s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   1.4s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   1.7s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   1.7s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   1.3s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   1.4s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   1.4s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   1.3s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   1.5s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   1.6s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   1.6s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   2.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   2.0s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.7s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.5s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   2.1s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.6s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.6s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   1.4s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   1.3s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   1.5s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   1.3s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   1.3s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   1.3s\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best Cross-Validation Score: 0.8106275816122585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the SVM model\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
    "    'kernel': ['linear', 'rbf'],  # Different kernel types\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with SVM model and parameter grid\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.806070826306914\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       398\n",
      "           1       0.73      0.65      0.69       195\n",
      "\n",
      "    accuracy                           0.81       593\n",
      "   macro avg       0.78      0.77      0.77       593\n",
      "weighted avg       0.80      0.81      0.80       593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the best model on the entire training data\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the accuracy and print the classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy Score: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class 0 (not kids safe):\n",
    "- Precision: 84% â€” The model is fairly good at identifying content that is \"not kids safe.\"\n",
    "- Recall: 88% â€” It correctly identifies most \"not kids safe\" content, but some content might still be misclassified.\n",
    "\n",
    "#### Class 1 (kids safe):\n",
    "- Precision: 73% â€” The model is moderately good at identifying content that is \"kids safe.\"\n",
    "- Recall: 65% â€” Thereâ€™s a need for improvement in identifying all \"kids safe\" content without missing too many.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a BERT model for the classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Hugging Face Transformers and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shusritavenugopal/Library/Python/3.12/lib/python/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shusritavenugopal/Library/Python/3.12/lib/python/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization\n",
    "Since BERT requires its input to be tokenized, we'll use the BERT tokenizer to convert your text data into the format expected by the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['combined_text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize the train and test data\n",
    "X_train_tfidf = df['combined_text'].tolist()\n",
    "y_train_tfidf = df['kids_safe_nltk'].tolist()\n",
    "\n",
    "# Splitting data for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, y_train_tfidf, test_size=0.2)\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert to PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, y_train)\n",
    "val_dataset = TextDataset(val_encodings, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Trainer Setup\n",
    "We'll use Hugging Face's Trainer API to train the model. We'll define the training arguments (like learning rate, batch size, number of epochs, etc.) and set up the Trainer with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.10.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/shusritavenugopal/Library/Python/3.12/lib/python/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.8.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers[torch] in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shusritavenugopal/Library/Python/3.12/lib/python/site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shusritavenugopal/Library/Python/3.12/lib/python/site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (2.5.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.8.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->transformers[torch]) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"accelerate>=0.26.0\"\n",
    "!pip install --upgrade \"transformers[torch]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n",
      "4.47.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "print(accelerate.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=4,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "## This configurations gave 83% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=4,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.05,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "## This configurations gave 83% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=5,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "## This configurations yielded 82% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=6,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "## This configurations gave 80% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated hugging face transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': accuracy_score(p.predictions.argmax(axis=1), p.label_ids)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n",
    "Now we can start training the model. This will take a while depending on the dataset size and the system you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ace03c0d2f4ef5ac5685e2d698a5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6936, 'grad_norm': 3.143017053604126, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 0.6891, 'grad_norm': 4.33994197845459, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.13}\n",
      "{'loss': 0.6756, 'grad_norm': 3.7533223628997803, 'learning_rate': 3e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6388, 'grad_norm': 3.5352041721343994, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.27}\n",
      "{'loss': 0.6648, 'grad_norm': 3.93685245513916, 'learning_rate': 5e-06, 'epoch': 0.34}\n",
      "{'loss': 0.6067, 'grad_norm': 2.4190571308135986, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 0.5951, 'grad_norm': 3.4141294956207275, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.47}\n",
      "{'loss': 0.5977, 'grad_norm': 3.3383634090423584, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.54}\n",
      "{'loss': 0.5453, 'grad_norm': 4.20260763168335, 'learning_rate': 9e-06, 'epoch': 0.6}\n",
      "{'loss': 0.5088, 'grad_norm': 6.475742816925049, 'learning_rate': 1e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6128, 'grad_norm': 8.684283256530762, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5719, 'grad_norm': 4.048276424407959, 'learning_rate': 1.2e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5288, 'grad_norm': 5.028045177459717, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6081, 'grad_norm': 2.3712992668151855, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5418, 'grad_norm': 6.512019634246826, 'learning_rate': 1.5e-05, 'epoch': 1.01}\n",
      "{'loss': 0.543, 'grad_norm': 4.292219638824463, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.07}\n",
      "{'loss': 0.4805, 'grad_norm': 3.6343424320220947, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.14}\n",
      "{'loss': 0.4289, 'grad_norm': 8.503642082214355, 'learning_rate': 1.8e-05, 'epoch': 1.21}\n",
      "{'loss': 0.5232, 'grad_norm': 7.990572929382324, 'learning_rate': 1.9e-05, 'epoch': 1.28}\n",
      "{'loss': 0.5116, 'grad_norm': 3.25205397605896, 'learning_rate': 2e-05, 'epoch': 1.34}\n",
      "{'loss': 0.494, 'grad_norm': 3.9338181018829346, 'learning_rate': 2.1e-05, 'epoch': 1.41}\n",
      "{'loss': 0.4069, 'grad_norm': 4.224359035491943, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.48}\n",
      "{'loss': 0.4957, 'grad_norm': 8.196895599365234, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.54}\n",
      "{'loss': 0.4155, 'grad_norm': 5.696361064910889, 'learning_rate': 2.4e-05, 'epoch': 1.61}\n",
      "{'loss': 0.4927, 'grad_norm': 6.332839012145996, 'learning_rate': 2.5e-05, 'epoch': 1.68}\n",
      "{'loss': 0.5178, 'grad_norm': 5.991470813751221, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.74}\n",
      "{'loss': 0.4626, 'grad_norm': 4.489915370941162, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.81}\n",
      "{'loss': 0.4636, 'grad_norm': 7.9966301918029785, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.88}\n",
      "{'loss': 0.4239, 'grad_norm': 4.4822282791137695, 'learning_rate': 2.9e-05, 'epoch': 1.95}\n",
      "{'loss': 0.4524, 'grad_norm': 5.9807868003845215, 'learning_rate': 3e-05, 'epoch': 2.01}\n",
      "{'loss': 0.3633, 'grad_norm': 8.667509078979492, 'learning_rate': 3.1e-05, 'epoch': 2.08}\n",
      "{'loss': 0.2876, 'grad_norm': 5.291843891143799, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.15}\n",
      "{'loss': 0.4065, 'grad_norm': 7.8806891441345215, 'learning_rate': 3.3e-05, 'epoch': 2.21}\n",
      "{'loss': 0.3312, 'grad_norm': 13.01650619506836, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.28}\n",
      "{'loss': 0.3812, 'grad_norm': 11.956616401672363, 'learning_rate': 3.5e-05, 'epoch': 2.35}\n",
      "{'loss': 0.3719, 'grad_norm': 11.3450288772583, 'learning_rate': 3.6e-05, 'epoch': 2.42}\n",
      "{'loss': 0.3166, 'grad_norm': 6.402215480804443, 'learning_rate': 3.7e-05, 'epoch': 2.48}\n",
      "{'loss': 0.3977, 'grad_norm': 9.01203441619873, 'learning_rate': 3.8e-05, 'epoch': 2.55}\n",
      "{'loss': 0.3451, 'grad_norm': 8.223573684692383, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.62}\n",
      "{'loss': 0.3856, 'grad_norm': 8.128595352172852, 'learning_rate': 4e-05, 'epoch': 2.68}\n",
      "{'loss': 0.4846, 'grad_norm': 5.935776233673096, 'learning_rate': 4.1e-05, 'epoch': 2.75}\n",
      "{'loss': 0.3381, 'grad_norm': 4.444345951080322, 'learning_rate': 4.2e-05, 'epoch': 2.82}\n",
      "{'loss': 0.3652, 'grad_norm': 4.599944591522217, 'learning_rate': 4.3e-05, 'epoch': 2.89}\n",
      "{'loss': 0.3193, 'grad_norm': 8.748221397399902, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.95}\n",
      "{'loss': 0.3409, 'grad_norm': 7.129025459289551, 'learning_rate': 4.5e-05, 'epoch': 3.02}\n",
      "{'loss': 0.2081, 'grad_norm': 11.551175117492676, 'learning_rate': 4.600000000000001e-05, 'epoch': 3.09}\n",
      "{'loss': 0.261, 'grad_norm': 11.631342887878418, 'learning_rate': 4.7e-05, 'epoch': 3.15}\n",
      "{'loss': 0.2071, 'grad_norm': 9.201830863952637, 'learning_rate': 4.8e-05, 'epoch': 3.22}\n",
      "{'loss': 0.205, 'grad_norm': 1.2015684843063354, 'learning_rate': 4.9e-05, 'epoch': 3.29}\n",
      "{'loss': 0.2167, 'grad_norm': 18.07434844970703, 'learning_rate': 5e-05, 'epoch': 3.36}\n",
      "{'loss': 0.2871, 'grad_norm': 12.685273170471191, 'learning_rate': 4.873096446700508e-05, 'epoch': 3.42}\n",
      "{'loss': 0.1329, 'grad_norm': 3.6808605194091797, 'learning_rate': 4.746192893401015e-05, 'epoch': 3.49}\n",
      "{'loss': 0.1934, 'grad_norm': 9.902847290039062, 'learning_rate': 4.619289340101523e-05, 'epoch': 3.56}\n",
      "{'loss': 0.3436, 'grad_norm': 12.9382963180542, 'learning_rate': 4.492385786802031e-05, 'epoch': 3.62}\n",
      "{'loss': 0.1808, 'grad_norm': 6.258123874664307, 'learning_rate': 4.365482233502538e-05, 'epoch': 3.69}\n",
      "{'loss': 0.2566, 'grad_norm': 9.47140121459961, 'learning_rate': 4.238578680203046e-05, 'epoch': 3.76}\n",
      "{'loss': 0.2615, 'grad_norm': 8.564992904663086, 'learning_rate': 4.111675126903554e-05, 'epoch': 3.83}\n",
      "{'loss': 0.194, 'grad_norm': 7.442049980163574, 'learning_rate': 3.9847715736040605e-05, 'epoch': 3.89}\n",
      "{'loss': 0.2654, 'grad_norm': 3.798940420150757, 'learning_rate': 3.8578680203045685e-05, 'epoch': 3.96}\n",
      "{'loss': 0.1967, 'grad_norm': 7.180728435516357, 'learning_rate': 3.7309644670050766e-05, 'epoch': 4.03}\n",
      "{'loss': 0.1237, 'grad_norm': 2.6458311080932617, 'learning_rate': 3.604060913705584e-05, 'epoch': 4.09}\n",
      "{'loss': 0.0979, 'grad_norm': 1.3915444612503052, 'learning_rate': 3.477157360406091e-05, 'epoch': 4.16}\n",
      "{'loss': 0.076, 'grad_norm': 12.824868202209473, 'learning_rate': 3.3502538071065994e-05, 'epoch': 4.23}\n",
      "{'loss': 0.1155, 'grad_norm': 6.528877258300781, 'learning_rate': 3.223350253807107e-05, 'epoch': 4.3}\n",
      "{'loss': 0.0845, 'grad_norm': 1.00483238697052, 'learning_rate': 3.096446700507614e-05, 'epoch': 4.36}\n",
      "{'loss': 0.0391, 'grad_norm': 0.16040608286857605, 'learning_rate': 2.969543147208122e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0824, 'grad_norm': 0.7059348225593567, 'learning_rate': 2.84263959390863e-05, 'epoch': 4.5}\n",
      "{'loss': 0.0567, 'grad_norm': 1.462730884552002, 'learning_rate': 2.715736040609137e-05, 'epoch': 4.56}\n",
      "{'loss': 0.1516, 'grad_norm': 0.2278997004032135, 'learning_rate': 2.588832487309645e-05, 'epoch': 4.63}\n",
      "{'loss': 0.1544, 'grad_norm': 13.193624496459961, 'learning_rate': 2.4619289340101523e-05, 'epoch': 4.7}\n",
      "{'loss': 0.1634, 'grad_norm': 58.88581466674805, 'learning_rate': 2.33502538071066e-05, 'epoch': 4.77}\n",
      "{'loss': 0.0824, 'grad_norm': 15.887879371643066, 'learning_rate': 2.2081218274111677e-05, 'epoch': 4.83}\n",
      "{'loss': 0.0875, 'grad_norm': 13.338950157165527, 'learning_rate': 2.0812182741116754e-05, 'epoch': 4.9}\n",
      "{'loss': 0.1964, 'grad_norm': 1.8475888967514038, 'learning_rate': 1.9543147208121827e-05, 'epoch': 4.97}\n",
      "{'loss': 0.1177, 'grad_norm': 0.39307570457458496, 'learning_rate': 1.8274111675126904e-05, 'epoch': 5.03}\n",
      "{'loss': 0.0311, 'grad_norm': 11.929020881652832, 'learning_rate': 1.700507614213198e-05, 'epoch': 5.1}\n",
      "{'loss': 0.0222, 'grad_norm': 3.5622665882110596, 'learning_rate': 1.5736040609137055e-05, 'epoch': 5.17}\n",
      "{'loss': 0.007, 'grad_norm': 0.11653240770101547, 'learning_rate': 1.4467005076142132e-05, 'epoch': 5.23}\n",
      "{'loss': 0.0212, 'grad_norm': 0.6388550996780396, 'learning_rate': 1.3197969543147209e-05, 'epoch': 5.3}\n",
      "{'loss': 0.0536, 'grad_norm': 0.06287100166082382, 'learning_rate': 1.1928934010152284e-05, 'epoch': 5.37}\n",
      "{'loss': 0.0186, 'grad_norm': 0.7755765318870544, 'learning_rate': 1.0659898477157361e-05, 'epoch': 5.44}\n",
      "{'loss': 0.0563, 'grad_norm': 0.307853102684021, 'learning_rate': 9.390862944162437e-06, 'epoch': 5.5}\n",
      "{'loss': 0.0258, 'grad_norm': 0.3074234127998352, 'learning_rate': 8.121827411167512e-06, 'epoch': 5.57}\n",
      "{'loss': 0.0155, 'grad_norm': 0.37941479682922363, 'learning_rate': 6.852791878172589e-06, 'epoch': 5.64}\n",
      "{'loss': 0.0384, 'grad_norm': 9.37717056274414, 'learning_rate': 5.583756345177665e-06, 'epoch': 5.7}\n",
      "{'loss': 0.0473, 'grad_norm': 0.40603524446487427, 'learning_rate': 4.3147208121827415e-06, 'epoch': 5.77}\n",
      "{'loss': 0.0076, 'grad_norm': 12.673585891723633, 'learning_rate': 3.0456852791878177e-06, 'epoch': 5.84}\n",
      "{'loss': 0.0268, 'grad_norm': 0.06585551798343658, 'learning_rate': 1.7766497461928936e-06, 'epoch': 5.91}\n",
      "{'loss': 0.0476, 'grad_norm': 0.05067836120724678, 'learning_rate': 5.076142131979695e-07, 'epoch': 5.97}\n",
      "{'train_runtime': 835.323, 'train_samples_per_second': 17.031, 'train_steps_per_second': 1.07, 'train_loss': 0.30296231862845135, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=894, training_loss=0.30296231862845135, metrics={'train_runtime': 835.323, 'train_samples_per_second': 17.031, 'train_steps_per_second': 1.07, 'total_flos': 935754468387840.0, 'train_loss': 0.30296231862845135, 'epoch': 6.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ef889e201f47c7a6216549718758a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.949877917766571, 'eval_accuracy': 0.8347386172006745, 'eval_runtime': 11.8746, 'eval_samples_per_second': 49.938, 'eval_steps_per_second': 0.842, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45a009ea70d4e31a8339e69bed00e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       402\n",
      "           1       0.77      0.69      0.73       191\n",
      "\n",
      "    accuracy                           0.83       593\n",
      "   macro avg       0.82      0.80      0.81       593\n",
      "weighted avg       0.83      0.83      0.83       593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's predict on the validation dataset first\n",
    "predictions = trainer.predict(val_dataset)\n",
    "predicted_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_val, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Accuracy:\n",
    "The model achieved an accuracy of 83%, which is respectable for a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ../saved_BERT_pretrained_models/model_20241211_113756_v818574\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a unique model name\n",
    "model_name = f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}_v839069\"\n",
    "\n",
    "# Save the model with the generated name\n",
    "save_path = os.path.join(\"../saved_BERT_pretrained_models\", model_name)\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "print(f\"Model saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ../saved_BERT_pretrained_models/model_20241211_133348_v839069\n",
      "Model and classification report saved in '../saved_BERT_pretrained_models'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Specify the folder to save the model and report\n",
    "model_folder = \"../saved_BERT_pretrained_models\"\n",
    "# Generate a unique model name\n",
    "model_name = f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}_v839069\"\n",
    "\n",
    "# Save the model with the generated name\n",
    "save_path = os.path.join(model_folder, model_name)\n",
    "trainer.save_model(save_path)\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved at: {save_path}\")\n",
    "# Generate and save the classification report\n",
    "report = classification_report(y_val, predicted_labels)\n",
    "report_path = os.path.join(model_folder, \"classification_report.txt\")\n",
    "\n",
    "with open(report_path, \"w\") as file:\n",
    "    file.write(report)\n",
    "\n",
    "print(f\"Model and classification report saved in '{model_folder}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
